You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.13it/s]
trainable params: 1,843,200 || all params: 3,756,466,176 || trainable%: 0.0491
  0%|                                                                                                   | 0/4750 [00:00<?, ?it/s]/home/jiyoon/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/jiyoon/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  0%|▏                                                                                      | 13/4750 [05:00<28:15:44, 21.48s/it]Traceback (most recent call last):
{'loss': -0.0, 'grad_norm': 0.283833771944046, 'learning_rate': 9.981052631578949e-06, 'num_tokens': 149349.0, 'completions/mean_length': 157.8625, 'completions/min_length': 83.1, 'completions/max_length': 361.9, 'completions/clipped_ratio': 0.0375, 'completions/mean_terminated_length': 144.94464492797852, 'completions/min_terminated_length': 83.1, 'completions/max_terminated_length': 296.7, 'rewards/f/mean': 0.1782793395172727, 'rewards/f/std': 0.10497283294486502, 'reward': 0.5348379969596863, 'reward_std': 0.2704672336578369, 'frac_reward_zero_std': 0.15, 'entropy': 0.09526150748133659, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0}
  File "/home/jiyoon/LViton_GRPO/train.py", line 133, in <module>
    trainer.train()
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3790, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1258, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1574, in _generate_and_score_completions
    prompt_completion_ids = unwrapped_model.generate(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2634, in generate
    result = self._sample(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 3660, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/jiyoon/LViton_GRPO/train.py", line 133, in <module>
    trainer.train()
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3790, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1258, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1574, in _generate_and_score_completions
    prompt_completion_ids = unwrapped_model.generate(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/peft/peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2634, in generate
    result = self._sample(
  File "/home/jiyoon/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 3660, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
